# Political Monitoring Agent v0.2.0 Configuration
# Copy this file to .env and configure your API keys and settings

# ============================================================================
# CORE CONFIGURATION
# ============================================================================

# === Environment ===
ENVIRONMENT=development
LOG_LEVEL=INFO

# ============================================================================
# DATABASE & SERVICES  
# ============================================================================

# === PostgreSQL (for Langfuse only) ===
# PostgreSQL runs in Docker via docker-compose
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# === Neo4j (for Graphiti Knowledge Graph) ===
# Neo4j runs in Docker via docker-compose
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123
GRAPHITI_GROUP_ID=political_monitoring_v2

# ============================================================================
# LLM & AI CONFIGURATION
# ============================================================================

# === LLM Settings ===
LLM_ENABLED=false  # Set to true when you have API keys configured
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

# === OpenAI Configuration ===
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4

# === Anthropic Configuration ===
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# === Langfuse (LLM Observability) ===
# SETUP INSTRUCTIONS:
# 1. Run: just services-up
# 2. Visit: http://localhost:3001
# 3. Sign up with any email/password
# 4. Create organization and project
# 5. Go to Settings → API Keys → Create new API key
# 6. Replace the values below with your actual keys

LANGFUSE_PUBLIC_KEY=pk-lf-your-actual-public-key-from-ui
LANGFUSE_SECRET_KEY=sk-lf-your-actual-secret-key-from-ui
LANGFUSE_HOST=http://localhost:3001

# ============================================================================
# NEWS COLLECTION (ETL)
# ============================================================================

# === News Collectors ===
# Configure one or both API keys for automated news collection

# Apify (RSS headlines and summaries)
# Get your API key from: https://console.apify.com/account/integrations
APIFY_API_TOKEN=your_apify_api_token_here

# Exa.ai (full article content - RECOMMENDED)
# Get your API key from: https://dashboard.exa.ai/api-keys
EXA_API_KEY=your_exa_api_key_here

# Default collector: exa_direct (recommended), apify, or exa
NEWS_COLLECTOR=exa_direct

# === ETL Initialization Settings ===
# Configure historical data collection for first-time setup

# Days to look back on FIRST RUN (initialization)
# Common values: 30 (1 month), 90 (3 months), 180 (6 months)
ETL_INITIALIZATION_DAYS=30

# Days to look back on REGULAR DAILY RUNS
# Typically 1 for daily collection, 2-3 for safety overlap
ETL_DAILY_COLLECTION_DAYS=1

# How it works:
# - First time: Collects ETL_INITIALIZATION_DAYS worth of articles
# - After that: Collects ETL_DAILY_COLLECTION_DAYS worth of articles
# - Status tracked per collector type automatically
# - Reset with: just etl-reset <collector>

# ============================================================================
# STORAGE CONFIGURATION
# ============================================================================

# === Azure Storage ===
# For development: Use Azurite (local emulator)
# For production: Update connection string to real Azure Storage

USE_AZURE_STORAGE=false  # Set to true for Azure mode

# Azurite connection string (for local development)
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://localhost:10000/devstoreaccount1;

# === Local Storage Paths ===
# Used when USE_AZURE_STORAGE=false
DEFAULT_INPUT_FOLDER=./data/input
DEFAULT_OUTPUT_FOLDER=./data/output
DEFAULT_CONTEXT_FOLDER=./data/context

# ============================================================================
# PROCESSING SETTINGS
# ============================================================================

# === Document Processing ===
PROCESSING_TIMEOUT_SECONDS=600
MAX_DOCUMENT_SIZE_MB=50
PRIORITY_THRESHOLD=70.0
CONFIDENCE_THRESHOLD=0.7

# === Ray Configuration ===
# Ray handles distributed processing
RAY_ADDRESS=auto  # Use local Ray cluster
RAY_NUM_CPUS=4    # Adjust based on your machine

# ============================================================================
# QUICK START GUIDE
# ============================================================================

# FIRST TIME SETUP:
# 1. Copy this file: cp .env.template .env
# 2. Get Exa.ai API key: https://dashboard.exa.ai/api-keys
# 3. Update EXA_API_KEY above
# 4. Run: just setup && just services-up && just dev
# 5. Configure Langfuse at: http://localhost:3001
# 6. Test at: http://localhost:3370 (admin/admin)

# DAILY WORKFLOW:
# - Start: just dev
# - Test: http://localhost:3370
# - ETL: just airflow-trigger-news
# - Monitor: http://localhost:8080 (Airflow UI)

# ============================================================================
# PRODUCTION OVERRIDES
# ============================================================================

# For production deployment, create a separate .env.production file with:
# ENVIRONMENT=production
# LLM_ENABLED=true
# USE_AZURE_STORAGE=true
# AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=...
# LANGFUSE_HOST=https://cloud.langfuse.com
# NEO4J_URI=bolt://your-neo4j-server:7687