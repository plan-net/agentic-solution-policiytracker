# v0.2.0 Split Graph Processing Architecture with Graphiti

## Executive Summary

This document outlines the architecture for evolving the Political Monitoring Agent from a single-pass document analysis system to a sophisticated temporal knowledge graph platform powered by Graphiti. The system will implement three distinct Kodosumi applications:

1. **Data Ingestion Workflow** - Daily document processing with LLM-driven entity extraction and web research
2. **Company Context Workflow** - Daily client context enrichment through web searches and knowledge updates
3. **Relevance Assessment Workflow** - Time-based analysis of new events and their relevance to clients

**Key Innovation**: Using Graphiti's temporal-aware knowledge graph with MCP server integration for true time-based intelligence.

## Current State → Future State

### Current Architecture (v0.1.0)
- Single workflow: Document → Score → Report
- Stateless processing (no persistent knowledge)
- Client context as static YAML file
- Rule-based + LLM hybrid scoring
- Neo4j GraphRAG experimental but not integrated

### Future Architecture (v0.2.0)
- Three independent Kodosumi apps on Ray cluster
- Temporal knowledge graph with Graphiti
- Dynamic client context with daily web research
- LLM-only approach (no rule-based logic)
- MCP server for clean API boundaries
- Time-aware queries and historical analysis

## Architectural Design

### Core Components

```
┌─────────────────────┐     ┌─────────────────────┐     ┌─────────────────────┐
│  Kodosumi App #1    │     │  Kodosumi App #2    │     │  Kodosumi App #3    │
│  Data Ingestion     │     │  Company Context    │     │ Relevance Assessment│
│                     │     │                     │     │                     │
│ • Document loader   │     │ • Client.yaml load  │     │ • Time-range query  │
│ • Entity extraction │     │ • Web research      │     │ • Event detection   │
│ • Web research     │     │ • Context docs      │     │ • Relevance scoring │
│ • Graph updates    │     │ • Graph enrichment  │     │ • Memory & reports  │
└──────────┬──────────┘     └──────────┬──────────┘     └──────────┬──────────┘
           │                            │                            │
           └────────────────────────────┴────────────────────────────┘
                                        │
                              ┌─────────▼─────────┐
                              │   Graphiti MCP    │
                              │     Server        │
                              │                   │
                              │ • Episode mgmt    │
                              │ • Entity ops      │
                              │ • Temporal query  │
                              │ • Group filtering │
                              └─────────┬─────────┘
                                        │
                              ┌─────────▼─────────┐
                              │   Neo4j with      │
                              │   Graphiti Core   │
                              │                   │
                              │ • Bi-temporal     │
                              │ • Episodic memory │
                              │ • Auto ontology   │
                              │ • Conflict res    │
                              └───────────────────┘
```

### 1. Data Ingestion Workflow (Kodosumi App #1)

**Purpose**: Daily processing of new documents with LLM entity extraction and web research enrichment

**Process Flow**:
1. **Document Loading**
   - Load files from `data/input/` folder
   - Track processed documents (only process new ones)
   - Support multiple formats (PDF, DOCX, MD, HTML, TXT)

2. **Entity Recognition Loop**
   - LLM extracts configurable entity types (politicians, laws, initiatives, etc.)
   - Web research across trusted sources for entity context
   - Re-run entity extraction on research results
   - Configurable iteration count (default: 1)

3. **Graph Update**
   - Chunk documents for processing
   - Generate embeddings
   - Create episodes in Graphiti
   - Update temporal knowledge graph

4. **Reporting**
   - Documents processed count
   - Entities recognized breakdown
   - Graph updates (nodes/relationships added)
   - Processing statistics

**Key Requirements**:
- Time-sensitive: Only process unseen documents
- Configurable entity types via config file
- Configurable trusted sources for web research
- LLM-only approach (no keyword matching)

### 2. Company Context Workflow (Kodosumi App #2)

**Purpose**: Daily enrichment of client context through web research and graph updates

**Process Flow**:
1. **Context Loading**
   - Load `client.yaml` as briefing document
   - Parse company terms, industries, markets, themes

2. **Web Research**
   - Execute targeted web searches about the client
   - Gather industry news, competitor updates, market trends
   - Collect regulatory developments in client's sectors

3. **Document Processing** (Reusable Component)
   - Process collected web documents
   - Extract entities and relationships
   - Same pipeline as Data Ingestion workflow

4. **Graph Enrichment**
   - Update client's graph group in Graphiti
   - Add new context nodes and relationships
   - Track temporal changes in client landscape

5. **Reporting**
   - Context updates summary
   - New entities/relationships for client
   - Graph statistics for client group

**Key Requirements**:
- Reuse document processing logic from Data Ingestion
- Client-specific graph updates (use Graphiti groups)
- Daily execution schedule
- Web search configuration

### 3. Relevance Assessment Workflow (Kodosumi App #3)

**Purpose**: Time-based analysis of new events and their relevance to clients

**Process Flow**:
1. **Time Range Selection**
   - User selects: last day, week, month, or all-time
   - Query Graphiti for temporal data in range

2. **Change Detection**
   - Identify new additions to knowledge graph
   - Detect updates to existing entities
   - Track new relationships formed

3. **Event Identification**
   - Extract events (news, laws, speeches, etc.)
   - Link events to client's topics of interest
   - Use graph traversal for impact analysis

4. **Relevance Scoring** (LLM-driven)
   - Assess time sensitivity of events
   - Determine direct vs indirect impact
   - Identify affected business areas
   - Generate relevance justifications

5. **Memory Management**
   - Load previous assessment summaries
   - Create new assessment summary
   - Store for future context
   - Build assessment history

6. **Event Reporting**
   - "What's New" event list
   - Relevance explanations per event
   - Priority ranking
   - Time sensitivity indicators

**Key Requirements**:
- Temporal queries via Graphiti
- LLM-based relevance assessment
- Assessment memory persistence
- No deep analysis (separate future workflow)
- Focus on filtering and ranking

## High-Level Changes Required

### 1. Infrastructure Changes

**Graphiti + Neo4j Setup**:
- Deploy Neo4j with Graphiti core library
- Run Graphiti MCP server in Docker
- Configure langchain-mcp-adapters
- Set up proper backup for temporal data

**Kodosumi Applications**:
- Create three separate Kodosumi apps
- Configure Ray cluster resources per app
- Set up independent deployment configs
- Enable app-specific scheduling

### 2. Data Model Evolution

**Graphiti Episode-Based Model**:
```python
# Episodes (Temporal Units)
- DocumentEpisode: Ingested documents with timestamps
- ResearchEpisode: Web research results with context
- AssessmentEpisode: Relevance assessments with memory

# Entity Types (Configurable)
- Politician(name, party, position, jurisdiction)
- Policy(title, type, status, effective_date)
- Organization(name, type, industry, size)
- Event(type, date, description, impact)
- Topic(name, category, client_relevance)

# Temporal Relationships
- MENTIONED_IN(episode_id, timestamp)
- AFFECTS(start_date, end_date, impact_level)
- SUPERSEDES(effective_date)
- RELATES_TO(confidence, context)
```

**Key Advantages**:
- Automatic temporal tracking
- Episode-based organization
- Built-in conflict resolution
- Query by time range

### 3. Integration Architecture

**Service Stack**:
```yaml
# docker-compose.yml additions
services:
  neo4j:
    image: neo4j:5-enterprise
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
  
  graphiti-mcp:
    image: graphiti-mcp-server:latest
    depends_on:
      - neo4j
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
```

**MCP Integration**:
- LangChain MCP adapters for workflow integration
- Graphiti MCP server for API access
- Clean separation of concerns
- Async operation support

### 4. Reusable Components

**Document Processing Pipeline**:
```python
# Shared between Data Ingestion and Company Context
class DocumentProcessor:
    - chunk_documents()
    - extract_entities()
    - generate_embeddings()
    - create_episodes()
    - update_graph()
```

**Web Research Module**:
```python
# Configurable research component
class WebResearcher:
    - search_trusted_sources()
    - validate_results()
    - extract_content()
    - filter_relevance()
```

## Questions and Decisions Needed

### 1. Technical Decisions

**Graphiti Implementation**:
- [ ] Docker deployment strategy for MCP server?
- [ ] Graphiti configuration for production scale?
- [ ] Backup strategy for temporal data?
- [ ] Performance tuning for large graphs?

**Kodosumi Apps Structure**:
- [ ] Separate repos or monorepo for three apps?
- [ ] Shared library for document processing?
- [ ] Ray resource allocation per app?
- [ ] Scheduling: Built-in vs external (Airflow)?

**Web Research Configuration**:
- [ ] Which trusted sources to configure?
- [ ] API keys needed (news APIs, search)?
- [ ] Rate limiting strategy?
- [ ] Research iteration limits?

### 2. Business Logic Decisions

**Entity Configuration**:
- [ ] Initial entity types to configure?
- [ ] Entity extraction prompt templates?
- [ ] Confidence thresholds for extraction?
- [ ] How to handle entity evolution over time?

**Assessment Memory**:
- [ ] Storage format for assessment summaries?
- [ ] Memory retention period?
- [ ] Summary generation prompts?
- [ ] Context window for historical assessments?

**Relevance Criteria**:
- [ ] Time sensitivity scoring factors?
- [ ] Direct vs indirect impact weights?
- [ ] Business area categorization?
- [ ] Event priority thresholds?

### 3. Implementation Strategy

**Phase 1 - Foundation** (Week 1-2):
- [ ] Set up Neo4j + Graphiti in Docker
- [ ] Deploy MCP server
- [ ] Create shared document processor
- [ ] Test Graphiti temporal queries

**Phase 2 - Data Ingestion** (Week 3-4):
- [ ] Build first Kodosumi app
- [ ] Implement entity extraction
- [ ] Add web research loop
- [ ] Test with sample documents

**Phase 3 - Company Context** (Week 5-6):
- [ ] Build second Kodosumi app
- [ ] Implement web search module
- [ ] Reuse document processor
- [ ] Test client enrichment

**Phase 4 - Relevance Assessment** (Week 7-8):
- [ ] Build third Kodosumi app
- [ ] Implement temporal queries
- [ ] Add assessment memory
- [ ] Test event detection

### 4. Validation Approach

**Step-by-Step Testing**:
- [ ] Unit tests for each component
- [ ] Integration tests with Graphiti
- [ ] End-to-end workflow tests
- [ ] Performance benchmarks

**Success Metrics**:
- [ ] Documents processed per hour
- [ ] Entity extraction accuracy
- [ ] Relevance assessment precision
- [ ] Query response times

## Next Steps

1. **Review and align** on this architecture with stakeholders
2. **Make key decisions** from the questions above
3. **Create detailed technical design** for chosen approach
4. **Build proof of concept** for highest-risk components
5. **Define implementation roadmap** with milestones
6. **Set up infrastructure** for development environment
7. **Begin iterative implementation** starting with MVP

## Appendix: Technology Stack

### Current Stack (Retained)
- Python 3.12.6
- Kodosumi v0.9.2 (UI and orchestration)
- Ray (distributed processing)
- Azure Storage (document storage)
- Langfuse (LLM observability)
- PostgreSQL (job tracking)

### New Additions
- Neo4j 5.x (graph database)
- Apache Airflow/Prefect (workflow scheduling)
- Redis (caching and queues)
- GraphQL (API layer)
- WebSockets (real-time updates)

### Enhanced Integrations
- neo4j-graphrag-python (advanced features)
- LangChain (expanded LLM tooling)
- Ray Data (distributed graph processing)
- Sentence Transformers (embeddings)
- spaCy/Hugging Face (NER models)